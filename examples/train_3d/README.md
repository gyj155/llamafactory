# 3D Vision-Language Model Training Examples

This directory contains example training configurations for training Qwen3-VL with 3D scene understanding capabilities.

## Overview

The 3D VLM training pipeline extends the standard vision-language training with:
- **3D World Coordinates**: Computed from depth maps and camera parameters
- **Sinusoidal 3D Positional Encoding**: Spatial awareness through sine/cosine encoding
- **MLP-based 3D Encoding**: Learnable spatial representations
- **Modular Design**: Easy to switch between different 3D processing methods

## Prerequisites

1. **Prepared Dataset**: Run the data preparation script first:
   ```bash
   cd /tmp3/yejie/llamafactory/data-3dllm
   python scripts/prepare_dataset.py
   ```

   This will generate:
   - `merged_training_data.json`: Combined dataset in alpaca format
   - `scene_metadata.pkl`: Scene metadata (camera intrinsics, axis alignment, etc.)

2. **Directory Structure**:
   ```
   data-3dllm/
   ├── merged_training_data.json  # Generated by prepare_dataset.py
   ├── scene_metadata.pkl          # Generated by prepare_dataset.py
   ├── posed_images/               # RGB images, depth maps, camera poses
   │   ├── scene0000_00/
   │   │   ├── 00000.jpg          # RGB image
   │   │   ├── 00000.png          # Depth map
   │   │   ├── 00000.txt          # Camera pose (4x4 matrix)
   │   │   └── ...
   │   └── ...
   └── embodiedscan/               # Scene metadata
       ├── embodiedscan_infos_train.pkl
       ├── embodiedscan_infos_val.pkl
       └── embodiedscan_infos_test.pkl
   ```

## Training Configurations

### 1. Baseline (No 3D Features)
**Config**: `qwen3vl_3d_lora_sft.yaml`

Standard multi-image VQA training without 3D features. Use this as a baseline for comparison.

```bash
llamafactory-cli train examples/train_3d/qwen3vl_3d_lora_sft.yaml
```

**Key settings**:
- `use_3d_features: false`
- `feature_processing_method: base`

### 2. Sinusoidal 3D Positional Encoding
**Config**: `qwen3vl_3d_sin3dpe_lora_sft.yaml`

Injects 3D spatial awareness using sinusoidal positional encoding of world coordinates.

```bash
llamafactory-cli train examples/train_3d/qwen3vl_3d_sin3dpe_lora_sft.yaml
```

**Key settings**:
- `use_3d_features: true`
- `feature_processing_method: sin3d_pe`
- `position_encoding_dim: 3584`

**How it works**:
1. DataCollator loads depth maps and camera poses
2. Computes 3D world coordinates via unprojection
3. Encodes coordinates with sine/cosine functions
4. Adds encoded positions to image features

### 3. MLP-based 3D Encoding (Custom)
**Config**: Create `qwen3vl_3d_mlp_lora_sft.yaml` with:

```yaml
use_3d_features: true
feature_processing_method: mlp_pe
```

Uses learnable MLPs to encode 3D coordinates.

## Feature Processing Methods

### Base (`base`)
No 3D processing. Standard vision-language training.

### Sinusoidal 3D PE (`sin3d_pe`)
Encodes (x, y, z) coordinates using sine and cosine functions:
```python
PE(x, y, z) = concat(
    [sin(x/T^(2i/d)), cos(x/T^(2i/d))],
    [sin(y/T^(2i/d)), cos(y/T^(2i/d))],
    [sin(z/T^(2i/d)), cos(z/T^(2i/d))]
)
```

### MLP-based PE (`mlp_pe`)
Learns to encode coordinates through MLPs:
```python
PE(x, y, z) = MLP([x, y, z])
```

## Customization

### Adding a New Feature Processing Method

1. **Implement the processor** in `src/llamafactory/model/qwen3_vl/feature_processing_3d.py`:
   ```python
   class CustomProcessor(nn.Module):
       def __init__(self, config):
           super().__init__()
           # Your initialization
       
       def forward(self, image_features, world_coords=None, depths=None, **kwargs):
           # Your processing logic
           return enhanced_features
   ```

2. **Register in factory function**:
   ```python
   def get_feature_processor(config):
       # ... existing code ...
       elif method == 'custom':
           return CustomProcessor(config)
   ```

3. **Use in training config**:
   ```yaml
   use_3d_features: true
   feature_processing_method: custom
   ```

## Monitoring Training

Check TensorBoard logs:
```bash
tensorboard --logdir=saves/qwen3vl-3d-sin3dpe-lora-sft/logs
```

Key metrics to monitor:
- Training loss
- Validation loss
- Learning rate schedule

## Troubleshooting

### Issue: "No metadata for scene X"
**Solution**: Ensure the scene exists in `scene_metadata.pkl`. Re-run `prepare_dataset.py` if needed.

### Issue: "Depth file not found"
**Solution**: Verify `posed_images/` directory structure. Each scene should have `.jpg`, `.png`, and `.txt` files for each frame.

### Issue: Out of memory
**Solution**: Reduce batch size or gradient accumulation steps:
```yaml
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
```

### Issue: 3D features not being used
**Solution**: Ensure `use_3d_features: true` in the config and check logs for "Loaded scene metadata" message.

## Performance Tips

1. **Use BF16**: Enables efficient training on modern GPUs
   ```yaml
   bf16: true
   ```

2. **Optimize batch size**: Balance between GPU memory and training speed
   ```yaml
   per_device_train_batch_size: 1
   gradient_accumulation_steps: 16
   ```

3. **Enable gradient checkpointing**: For larger models (add to config if needed)
   ```yaml
   gradient_checkpointing: true
   ```

## Citation

If you use this 3D VLM training pipeline, please cite:
```bibtex
@misc{3dvlm_llamafactory,
  title={3D Vision-Language Model Training Pipeline},
  author={Your Name},
  year={2024},
  howpublished={\url{https://github.com/your-repo}}
}
```

## References

- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- [Qwen3-VL](https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct)
- [ScanNet Dataset](http://www.scan-net.org/)
- [Attention is All You Need](https://arxiv.org/abs/1706.03762) - Positional Encoding

